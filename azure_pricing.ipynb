{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "azure_pricing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbEmK0K8okt7M9Dj39bsQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dspriggs-ds/general-notebooks/blob/main/azure_pricing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbvDfO69GcD"
      },
      "source": [
        "# Azure Price Data Pipeline\n",
        "\n",
        "## Overview\n",
        "\n",
        "The following pipeline retrieves Azure Pricing, by Region,  data from a Microsoft API. The data is then saved to a Delta Lake table and exported to an Excel spreadsheet. The following technologies are used:\n",
        "\n",
        "*   [Delta Lake](https://docs.microsoft.com/en-us/azure/databricks/delta/)\n",
        "*   [Python](https://www.python.org/)\n",
        "*   [Pandas](https://pandas.pydata.org/) \n",
        "*   [Jupyter Notebook](https://jupyter.org/)\n",
        "\n",
        "## Process\n",
        "\n",
        "The data pipeline connects to the [Microsoft Pricing API](https://docs.microsoft.com/en-us/rest/api/cost-management/retail-prices/azure-retail-prices#api-property-details) and retrieves the pricing data iteratively by region (see Overview for Azure regions). Next, the data is written to a Delta Lake table for storage. Last, the data is exported from the Delta Lake to an Excel workbook.\n",
        "\n",
        "## Reference\n",
        "https://github.com/santhoshkanala/azureretailpricesapi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3LGkL33djW"
      },
      "source": [
        "### Mount Google Drive for file storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Bj20P8tZku",
        "outputId": "497d7236-ed77-4df9-cf0a-c3c7cce0e0a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxEl7PEN3nxz"
      },
      "source": [
        "### Download then install Java and Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS9Xekd0qpl7"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72h1aA6HuR7p"
      },
      "source": [
        "!pip -q install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FCZhWt3segW"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ7S8GLy36uu"
      },
      "source": [
        "### Set environment variables to use Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJXYIMNhsarF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsauh0P44FNA"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghWMLBQkfH1w"
      },
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0tQCcoD4KW5"
      },
      "source": [
        "### Instantiate Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaBVa27CshVE"
      },
      "source": [
        "spark = SparkSession.builder.appName('delta_session').getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfQtfsrv4TE1"
      },
      "source": [
        "### Load Microsoft Azure pricing data for the US East, US East 2, US West and US West 2 Regions into JSON list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBwEuBvOgU01"
      },
      "source": [
        "regions = [\"eastus\",\"eastus2\",\"westus\",\"westus2\"]\n",
        "azurejslist = []\n",
        "\n",
        "for region in regions:\n",
        "\n",
        "  azureurl = \"https://prices.azure.com/api/retail/prices?$filter=armRegionName%20eq%20%27{0}%27\".format(region)\n",
        "  response = requests.get(azureurl)\n",
        "\n",
        "  while response.json()[\"NextPageLink\"] != None:\n",
        "    for i in response.json()['Items']:\n",
        "          azurejslist.append(i)\n",
        "    response = requests.get(response.json()[\"NextPageLink\"]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvlA8vKx7RwQ"
      },
      "source": [
        "### Load JSON List into Spark Dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQpglA1_k-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a68bb1-4812-4283-d54f-7e16513b44cd"
      },
      "source": [
        "azureprice_df = spark.createDataFrame(azurejslist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDUuhQB_7vdw"
      },
      "source": [
        "### Write Spark Dataframe to Delta Lake for storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LB1l4CJu7cv"
      },
      "source": [
        "azureprice_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(\"/content/drive/MyDrive/delta/azureprice\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPr3DpPG8AAE"
      },
      "source": [
        "### Read data from Delta Lake and export to Excel File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOYsgfh6iZzM"
      },
      "source": [
        "az_price_df = spark.read.format(\"delta\").load(\"/content/drive/MyDrive/delta/azureprice\").selectExpr(\"productName as Product\",\"skuName as SubProduct\",\"serviceName as Product_Type\",\"location as Location\",\"retailPrice as Price\", \"unitOfMeasure as Price_Unit_Of_Measure\")\n",
        "\n",
        "# az_price_df.toPandas().to_csv(\"/content/drive/MyDrive/output/azureprices.csv\",index=False)\n",
        "az_price_df.toPandas().to_excel(\"/content/drive/MyDrive/output/azureprices.xlsx\",sheet_name=\"azure_prices\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLkoTKg681Rd"
      },
      "source": [
        "### Clean up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfG4g-6Jrova"
      },
      "source": [
        "!rm -rf /content/spark-3.0.3-bin-hadoop3.2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}